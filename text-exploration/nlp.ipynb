{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tekstanalyse og enkel maskinlæring\n",
    "I denne notebooken skal vi gå gjennom hvordan man kan utforske tekstdata, og sette opp en enkel klassifisering av dataen.\n",
    "Den første oppgaven er å generere et datasett. Dette kan du enkelt gjøre ved å gi ChatGPT et par eksempel, og be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install plotly-express\n",
    "!pip install umap-learn\n",
    "!pip install scikit-learn\n",
    "!pip install nltk #natural language toolkit\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import plotly.express as px\n",
    "import umap.umap_ as umap\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "df_data = df['complaint']\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from the column values\n",
    "def remove_stopwords(values):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_values = []\n",
    "    for value in values:\n",
    "        words = value.split()\n",
    "        cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "        cleaned_values.append(' '.join(cleaned_words))\n",
    "    return cleaned_values\n",
    "\n",
    "column_values = df_data.tolist()\n",
    "cleaned_values = remove_stopwords(column_values)\n",
    "\n",
    "# Create Wordcloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "wordcloud.generate(' '.join(cleaned_values))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def generate_wordcloud_from_column(df):\n",
    "    # Get the column values as a list\n",
    "    column_values = df[\"complaint\"].tolist()\n",
    "\n",
    "    #Remove stopwords from the column values\n",
    "    cleaned_values = remove_stopwords(column_values)\n",
    "\n",
    "    # Combine two words separated by a space\n",
    "    word_pairs = []\n",
    "    for value in cleaned_values:\n",
    "        words = value.split()\n",
    "        pairs = [words[i] + ' ' + words[i+1] for i in range(len(words) - 1)]\n",
    "        word_pairs.extend(pairs)\n",
    "\n",
    "    # Create the word cloud from the frequencies\n",
    "    word_pair_freq = Counter(word_pairs)\n",
    "    wordcloud.generate_from_frequencies(dict(word_pair_freq))\n",
    "\n",
    "    # Display the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud_from_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "filter_word = \"login form\"\n",
    "\n",
    "df_filtered = df.loc[df.complaint.str.contains(filter_word)]\n",
    "generate_wordcloud_from_column(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(df_filtered.values[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "embeddings = model.encode(df_data)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def plot_word_embeddings(df, embeddings):\n",
    "    # Apply UMAP dimensionality reduction\n",
    "    umap_embeddings = umap.UMAP(n_components=2, metric='cosine', min_dist=0.0, random_state=10).fit(embeddings)\n",
    "\n",
    "    # Create a scatter plot using Plotly\n",
    "    fig = px.scatter(\n",
    "    x=umap_embeddings.embedding_[:,0], \n",
    "    y=umap_embeddings.embedding_[:,1], \n",
    "    color=df.category, \n",
    "    hover_data=[df.complaint],\n",
    "    color_discrete_sequence=px.colors.qualitative.Alphabet,\n",
    "    width=1000, height=800\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "plot_word_embeddings(df, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df.category, test_size=0.2, random_state=42)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "text_to_predict = \"When i signed into the page, i got an error message, and the page wouldn't load.\"\n",
    "cmp_pred = svc.predict(model.encode(text_to_predict).reshape(1, -1))\n",
    "print(cmp_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "text_to_predict = \"The text is too small\"\n",
    "cmp_pred = svc.predict(model.encode(text_to_predict).reshape(1, -1))\n",
    "print(cmp_pred[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det virker innledende ut som at modellen er veldig bra, men ettersom teksten er generert av ChatGPT så risikerer vi at dataen muligens ikke gjenspeiler ekte data, eller at de respektive kategoriene er veldig homogene, som gjør at det er enkelt å klassifisere. Her burde man aktivt gå gjennom dataen man har generert og forsikre seg at kvaliteten er god."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(filename, column_name):\n",
    "    df = pd.read_csv(filename)\n",
    "    duplicates = df[df.duplicated(subset=column_name, keep=False)]\n",
    "    return duplicates\n",
    "dupes = find_duplicates(\"data/dataset.csv\", \"complaint\")\n",
    "len(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se her ja! Det er en skjev fordelig i antall klager per kategori. La oss se hva som skjer når vi jevner det ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_equal = pd.read_csv(\"data/dataset_equal.csv\")\n",
    "\n",
    "embeddings = model.encode(df_equal.complaint)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df_equal.category, test_size=0.2, random_state=42)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da har vi ihvertfall fått jevnet ut kategoriene, men scoren er fremdeles mistenkelig høy. La oss se på hva de forskjellige kategoriene inneholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(df):\n",
    "    # Read the CSV file\n",
    "\n",
    "    # Separate complaints for each category\n",
    "    ux_complaints = df[df['category'] == 'UX']['complaint'].tolist()\n",
    "    page_error_complaints = df[df['category'] == 'page error']['complaint'].tolist()\n",
    "    ui_complaints = df[df['category'] == 'UI']['complaint'].tolist()\n",
    "\n",
    "    # Initialize TfidfVectorizer to convert text into numerical features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the complaints for each category\n",
    "    ux_tfidf_matrix = vectorizer.fit_transform(ux_complaints)\n",
    "    page_error_tfidf_matrix = vectorizer.transform(page_error_complaints)\n",
    "    ui_tfidf_matrix = vectorizer.transform(ui_complaints)\n",
    "\n",
    "    # Calculate similarity between different categories\n",
    "    ux_page_error_similarity = cosine_similarity(ux_tfidf_matrix, page_error_tfidf_matrix)\n",
    "    ux_ui_similarity = cosine_similarity(ux_tfidf_matrix, ui_tfidf_matrix)\n",
    "    page_error_ui_similarity = cosine_similarity(page_error_tfidf_matrix, ui_tfidf_matrix)\n",
    "\n",
    "    # Calculate average similarity within each category\n",
    "    ux_similarity = np.mean(cosine_similarity(ux_tfidf_matrix))\n",
    "    page_error_similarity = np.mean(cosine_similarity(page_error_tfidf_matrix))\n",
    "    ui_similarity = np.mean(cosine_similarity(ui_tfidf_matrix))\n",
    "\n",
    "    return {\n",
    "        \"UX similarity\": ux_similarity,\n",
    "        \"Page error similarity\": page_error_similarity,\n",
    "        \"UI similarity\": ui_similarity,\n",
    "        \"UX vs. Page error similarity\": np.mean(ux_page_error_similarity),\n",
    "        \"UX vs. UI similarity\": np.mean(ux_ui_similarity),\n",
    "        \"Page error vs. UI similarity\": np.mean(page_error_ui_similarity),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity within and between categories\n",
    "similarity_results = calculate_similarity(df_equal)\n",
    "\n",
    "# Print the similarity results\n",
    "for key, value in similarity_results.items():\n",
    "    print(key + \":\", value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ser vi ja at det er likheter innad i kategoriene, mens den er generelt lav mellom kategoriene. Om dette er noe som bør fikses eller ei er noe som man må utforske. Dette tilfellet er ekstra ekstremt med vilje, ettersom vi ikke ga noen informasjon om hvordan klagene skulle se ut slik at vi kunne utforske denne problemstillingen. I den ekte verden så er det jo tilfeller der kategoriene rett og slett er veldig adskilte. Food for thought ;)"
   ]
  }
 ],
 "metadata": {
  "description": null,
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "92a61a7787e1c15982bb3849fe07e0f8fce9a3f2f4316fcf414c2ad8bc037b7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
